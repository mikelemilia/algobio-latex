% Overview:
%   VarGeno TeX subfile for the project.
%   Each subfile MUST start with the following line
%		\documentclass[../main.tex]{subfiles}

\documentclass[../main.tex]{subfiles}

\begin{document}

\subsection{VarGeno}
\label{vargeno}
Volendo analizzare l'attuale stato dell'arte rispetto ai metodi di ricerca e genotipizzazione di SNP senza l'utilizzo dell'allineamento e \textit{reference based}, certamente un framework da presentare e studiare è VarGeno. Nel paper \cite{sun-medvedev2018vargeno}, viene presentato VarGeno, il sistema progettato per la genotipizzazione di SNP su dati provenienti dal sequenziamento dell'intero genoma. Esso si basa su LAVA \cite{shajii2016lava}, un precedente algoritmo di genotipizzazione per un assegnato set di loci SNP, il quale sfrutta il fatto che, in genere, la corrispondenza approssimativa di \textit{k}-mer di medie dimensioni è in grado di identificare univocamente i loci nel genoma umano senza un allineamento completo delle read. Partendo da LAVA, vengono migliorate la velocità di interrogazione dei \textit{k}-mer e l'accuratezza della strategia di genotipizzazione, arrivando con VarGeno ad ottenere prestazioni 7-13 volte più veloci di LAVA, con un utilizzo di memoria simile e migliorando allo stesso tempo la precisione.

VarGeno, differentemente dal suo predecessore, introduce una differente struttura dati, utile per indicizzare e interrogare i \textit{k}-mer, ossia il Bloom Filter (vedi sezione \ref{BloomFilter}), e nella maggioranza delle condizioni utilizza la scansione lineare per la ricerca all'interno dei blocchi. Inoltre, nella fase finale, quella in cui avviene la genotipizzazione, vengono utilizzati i valori di qualità e impiegati diversi criteri di mappatura delle read, per migliorare la velocità e l'accuratezza. 

L'algoritmo di base di VarGeno può essere visto come l'insieme delle seguenti componenti: l'algoritmo di base di LAVA, l'uso dei Bloom Filter, la scansione lineare, un cutoff dato dai valori di qualità e nuovi criteri di mappatura delle read. Inizieremo quindi con l'introdurre l'algoritmo di LAVA; seguirà un paragrafo per presentare i Bloom Filter e il loro funzionamento di base, per poi discutere di tutte le modifiche apportate a LAVA per ricostruire l'intero algoritmo di VarGeno. Viene presentato l'approccio di LAVA poiché costituisce un rilevante punto di partenza: alcune delle sue componenti verranno infatti riprese anche da altri framework che tratteremo in seguito.

\subsubsection{Algoritmo di LAVA}
\label{lava}
LAVA (\textbf{L}ightweight \textbf{A}ssignment of \textbf{V}ariant \textbf{A}lleles), è un framework che crea degli indici partendo dalla lista in input di SNP noti (ad es. dbSNP) e dalla sequenza di riferimento e utilizza poi una corrispondenza approssimativa dei \textit{k}-mer per genotipizzare il donatore dai dati di sequenziamento, usando le read grezze non ancora elaborate. Il framework prende in input il genoma di riferimento, una lista di SNP noti e un set di read e restituisce in output i genotipi predetti per gli SNP delle read. 

In particolare, data una lista di SNP, costruisce un dizionario completo di \textit{k}-mer (con \textit{k} = 32) che identificano in modo univoco quegli SNP (ove possibile). Abbinato ad un secondo dizionario di tutti i \textit{k}-mer nel genoma di riferimento, è in grado di determinare rapidamente se una read appartiene a un particolare SNP come allele selvatico (\textit{wild-type}) o allele mutato, attraverso una corrispondenza bipartita di \textit{k}-mer nelle read con i \textit{k}-mer nei dizionari precompilati, ammettendo fino ad una distanza di Hamming uguale a 1; non viene perciò eseguito nessun allineamento delle read. Inoltre, aggregando le read rilevanti, LAVA definisce gli SNP con un modello probabilistico usando la coverage prevista delle read e la frequenza a priori delle varianti nella lista di SNP in input.\\

\noindent
L'algoritmo usa come lunghezza di base dei \textit{k}-mer \textit{k} = 32 in quanto essi dovrebbero identificare in modo univoco i luoghi del genoma in cui si trovano gli SNP di interesse. La lunghezza scelta è adatta per rappresentare nelle attuali macchine i \textit{k}-mer, in quanto possono semplicemente essere codificati come numeri a 64 bit; tenendo inoltre conto dei possibili errori, \textit{k} dovrebbe essere sufficientemente grande per evitare che i \textit{k}-mer abbiano vicini di Hamming nel genoma. \cite{shajii2016lava} mostrano come, scegliendo \textit{k} = 32, si è in grado di localizzare in modo univoco l'85,7\% del genoma umano con corrispondenze esatte a 32-mer e il 79,3\% considerando anche i vicini di Hamming a distanza 1. Scegliendo un \textit{k} inferiore, la percentuale diminuisce e quindi non è conveniente; d'altro canto aumentando \textit{k}, la proporzione di loci univocamente identificabili aumenta, anche se \textit{k} = 32 sembra aver superato un punto di flesso e l'aumento poi non è considerevole. Un \textit{k} troppo elevato richiederebbe troppa memoria e comporterebbe un aumento della possibilità di errori di sequenziamento visto che l'algoritmo controlla anche i vicini di Hamming a distanza 1, correggendo un singolo errore, e guardare a distanze più alte richiederebbe esponenzialmente più tempo. Scegliendo \textit{k} = 32, si garantisce che raramente i \textit{k}-mer abbiano più di un errore.\\

\noindent
Procediamo con l'effettivo algoritmo. Nella fase di \textit{preprocessing}, si lavora sulla sequenza di riferimento, considerando ogni sottostringa di lunghezza \textit{k} che appare al suo interno, con l'obiettivo di creare un dizionario, $D_{ref}$, che associa ciascun \textit{k}-mer all'indice nella sequenza del genoma di riferimento in cui esso appare. $D_{ref}$ può essere trattato come un elenco di tuple \textlangle \textit{k}-mer, indice\textrangle, in cui il \textit{k}-mer è rappresentato dalla sua codifica intera senza segno a 64 bit. Per ottimizzare la ricerca, le tuple vengono ordinate secondo i valori dei \textit{k}-mer codificati e viene utilizzata una hash table, $J_{ref}$, che associa ogni intero senza segno a 32 bit \textit{u} alla prima posizione in $D_{ref}$ in cui c'è un 32-mer i cui 32 bit superiori della codifica coincidono con \textit{u}. Poiché $D_{ref}$ è ordinato in base ai valori numerici dei 32-mer codificati, i 32-mer le cui codifiche hanno gli stessi 32 bit superiori sono raggruppati in un bucket ordinato, diminuendo così gli errori e facilitando la ricerca di tutti i vicini di Hamming di un \textit{k}-mer: per effettuare la ricerca di un \textit{k}-mer in $D_{ref}$ viene individuata la posizione di partenza del bucket a cui appartiene, usando $J_{ref}$ (cercando la corrispondenza con i 32 bit superiori) e la posizione di partenza del bucket successivo (la posizione a cui punta il successivo elemento in $J_{ref}$) e viene eseguita una ricerca binaria nel bucket.

Per quanto riguarda la lista di SNP in input, ogni suo elemento è costituito dalla posizione nel riferimento in cui si trovano l'allele di riferimento, l'allele alternato e le frequenze a priori degli alleli nella popolazione. Essa viene pre-elaborata in modo analogo al genoma di riferimento: si costruisce un dizionario indice in cui vengono inseriti solamente i \textit{k}-mer che si sovrappongono ad alcuni SNP con l'allele alternato al posto dell'allele di riferimento. Il dizionario creato $D_{SNP}$ (così come l'hash table $J_{SNP}$) contiene i \textit{k}-mer con l'allele mutante e, oltre alle posizioni, include le informazioni sugli SNP. 
Per quanto riguarda la complessità di spazio delle strutture dati presentate, esse hanno dimensioni al massimo lineari nella taglia del genoma, poiché ogni locus nel genoma e ciascun SNP contribuiscono con un \textit{k}-mer e ogni \textit{k}-mer ha associate informazioni costanti.\\

\noindent
Riguardo all'elaborazione online delle read, al fine di determinarne le varianti, viene creata una \textit{pileup table}, P, in cui vengono memorizzati, per ogni SNP, i conteggi dell'allele di riferimento e dell'allele alternato che vengono aggiornati in modo incrementale durante l'elaborazione. La tabella può essere implementata per usare spazio lineare nella lunghezza del genoma o, usando una hash table, nel numero di SNP, quindi la complessità in spazio di LAVA è determinata dalle strutture statiche della fase di pre-processing.

L'elaborazione di una read inizia con la sua divisione in \textit{k}-mer contigui e non sovrapposti: l'obiettivo è determinare quale SNP, se presente, corrisponde alla lettura. Gli SNP vengono identificati interrogando i dizionari $D_{ref}$ e $D_{SNP}$, per ogni \textit{k}-mer campionato dalla read, e tutti i \textit{k}-mer a distanza di Hamming 1. Quindi, per ogni read, si identifica quale locus corrisponde e contiene il maggior numero di \textit{k}-mer in quella read. Se c'è più di un locus o se non ce n'è nessuno supportato da due o più \textit{k}-mer, P non viene aggiornata. Quando la procedura è stata applicata a tutte le read e la tabella P è compilata con i conteggi, l'ultimo passo è la determinazione degli SNP, che sfrutta un modello probabilistico: questo step è comune a molti altri framework, che nella loro fase finale sfruttano le probabilità per chiamare il genotipo. Viene assegnata un'etichetta ``omozigote di riferimento", ``eterozigote" o ``omozigote alternato" a ciascuna posizione in P, coperta da almeno una read poiché, avendo le frequenze degli alleli di riferimento e alternati, si può stimare la probabilità a posteriori di ogni genotipo, usando la regola di Bayes, e si determina come genotipo quello che ha la maggiore probabilità, dato l'evento osservato. \\

\noindent
Il framework LAVA si concentra sugli SNP ma è fattibile, secondo gli autori di  \cite{shajii2016lava}, pensare di estenderlo ed utilizzarlo anche per altre mutazioni, come piccoli indel (inserzioni e delezioni) o CNV (Copy Number Variation). Inoltre, una volta eseguita la fase di confronto dei \textit{k}-mer, è possibile applicare un numero qualsiasi di diversi modelli probabilistici per chiamare la base e determinare la più probabile. 

\subsubsection{Bloom Filter}
\label{BloomFilter}
Un Bloom Filter è una struttura dati probabilistica efficiente in termini di spazio, creata per migliorare la scalabilità \cite{sun-medvedev2018vargeno}. Per la loro semplicità ed efficacia sono usati per molteplici problemi in bioinformatica, come la rappresentazione dei grafi di \textit{Bruijn} (che tratteremo in seguito) e il conteggio di \textit{k}-mer in un campione. Il Bloom Filter rappresenta fondamentalmente un insieme di elementi e consente di effettuare query di appartenenza approssimative, per determinare cioè se un elemento appartiene o no all'insieme dei dati del Bloom Filter: il risultato di tali query può essere un falso positivo ma mai un falso negativo: questo significa che il filtro può verificare se un valore è ``possibilmente nel set" (ci possono essere casi in cui si pensa erroneamente che l'elemento appartenga) o ``sicuramente non nel set"; per questo motivo è chiamata struttura dati \textit{probabilistica}. Esso è molto efficiente in termini di spazio poiché, quando una tabella hash richiede di avere dimensioni arbitrarie in base ai dati di input, esso funziona bene con una dimensione fissa.


\theoremstyle{definition}
\begin{definition} 

Un Bloom Filter, che rappresenta un insieme di elementi \textit{S} di cardinalità \textit{n}, è un vettore di bit di lunghezza \textit{m} e un insieme di \textit{h} funzioni di hash \{$H_{1}$, ... , $H_{h}$\}, ognuna delle quali mappa ogni elemento dell'insieme in un valore indice \{1, ... , \textit{m}\}. 

\end{definition}

\noindent
Inizialmente tutti i bit del vettore sono impostati a 0. Quando un elemento \textit{e} viene aggiunto all'insieme, vengono impostati a 1 gli elementi delle celle di indice \{$H_{1}(e)$, ... , $H_{h}(e)$\}. Verificare che un elemento appartenga all'insieme significa controllare che tutte le posizioni \{$H_{1}(e)$, ... , $H_{h}(e)$\} siano 1: se anche una sola posizione è settata a 0, significa che l'elemento sicuramente non è nel set, altrimenti (se tutte le posizioni contengono il bit 1) probabilmente l'elemento vi appartiene \cite{compressedbloomfilter}. Il \textit{probabilmente} è dovuto al fatto che le celle potrebbero essere state settate a 1 a causa dell'inserimento di altri elementi, poiché le funzioni hash collidono. Si intuisce che quindi sono solo due le operazioni\footnote{\ L'operazione di rimozione non è supportata dalla versione standard, poiché settare a 0 tutti i bit presenti nelle posizioni relative alle funzioni di hash potrebbe annullare anche l'effetto dell'inserimento di altri elementi, di cui alcune funzioni di hash collidevano. In una possibile variante in cui si richiede di supportare la rimozione, chiamata \textit{Counting Bloom Filter}, si può pensare di memorizzare nelle varie posizioni un valore intero (e non solo bit): invece di contrassegnare il bit su "1" quando si inserisce un valore, si aumenta il valore intero di una unità; quando invece un elemento viene rimosso, il valore corrispettivo alle posizioni date dalle funzioni di hash viene ridotto di una unità. Per verificare l'esistenza di un elemento si controlla se gli indici corrispondenti alle funzioni hash contengono un valore maggiore di 0. Questa modifica al fine di supportare la funzionalità di rimozione comporta l'uso di più spazio.} supportate dai Bloom Filter, l'inserimento e il test per verificare l'appartenenza.

Concentrandoci sull'aspetto dei falsi positivi, possiamo dimostrare che dato un Bloom Filter di un set di \textit{n} elementi, con \textit{h} funzioni di hash e un vettore di \textit{m} bit, la probabilità che si verifichino è sufficientemente bassa ed è $(1 - e^{\frac{-hn}{m}})^{h} $; aumentando le dimensioni del filtro si riduce la percentuale di falsi positivi \cite{bernardini2019malva} \cite{compressedbloomfilter}. D'altro canto, se la dimensione è troppo piccola, i campi si riempiono velocemente e la percentuale di falsi positivi è molto alta, anche se si riduce l'utilizzo della memoria. In generale, più funzioni hash vengono usate, più lento è il filtro e più velocemente si riempie ma la percentuale di falsi positivi viene limitata; se ce ne sono troppo poche, tuttavia, potrebbero esserci troppo falsi positivi, anche se esiste una variante efficiente che utilizza una sola funzione di hash\footnote{ \label{nota:OHBF} \ L'\textit{One-Hashing Bloom Filter} (OHBF), è una variante dei Bloom Filter che ha lo scopo di abbassare il costo di calcolo che altrimenti potrebbe limitare le prestazioni: richiede solo una funzione hash di base più altre semplici operazioni. Pur mantenendo quasi lo stesso rapporto teorico di falsi positivi di un filtro Bloom standard, l'OHBF riduce significativamente il sovraccarico di calcolo dell'hash \cite{bloomonehash}. Questo risulta in un approccio molto efficiente con un confermato limite alle prestazioni. In \cite{bloomonehash} dimostrano che l'uso di molteplici buone funzioni di hash non porta necessariamente a migliori prestazioni rispetto ai falsi positivi, dando prova che l'OHBF può superare molte implementazioni pratiche di Bloom Filter con più di una funzione di hash. Nell'OHBF il processo di hashing genera ancora \textit{h} valori di hash come se ci fossero \textit{h} funzioni di hash indipendenti, ma tutti gli \textit{h} valori provengono da una singola funzione più alcune semplici operazioni di modulo, riducendo il costo di calcolo.}.

\subsubsection{Struttura dati di VarGeno}
VarGeno, usando come punto di partenza il metodo presentato da LAVA, utilizza una struttura dati differente per effettuare la genotipizzazione; data una serie di \textit{k}-mer \textit{S} con relativi dati aggiuntivi associati a ogni \textit{k}-mer, la struttura, definito un \textit{k}-mer \textit{K}, è in grado di restituire tutti i dati associati a tutti i \textit{k}-mer nel “vicinato di Hamming” del \textit{k}-mer \textit{K} che sono in \textit{S}. Il vicinato viene utilizzato, come precedentemente chiarito in LAVA, per consentire fino ad un errore di sequenziamento.

Viene scelto il valore \textit{k} = 32, come in LAVA, in modo che la probabilità che ci sia più di un nucleotide errato sia bassa e che un \textit{k}-mer possa essere codificato convenientemente con un numero intero a 64 bit. Inoltre, viene introdotto un nuovo parametro \textit{r} (che può variare tra 24-32 ma viene fissato \textit{r} = 32) che viene utilizzato per dividere il \textit{k}-mer codificato in \textit{r} bit superiori e (2\textit{k} – \textit{r}) bit inferiori. Vengono inoltre definiti, all'interno del vicinato \textit{N(K)}, il vicinato superiore, l'insieme di \textit{k}-mer la cui codifica differisce da \textit{K} negli \textit{r} bit superiori e il vicinato inferiore, l'insieme di \textit{k}-mer la cui codifica differisce da \textit{K} nei (2\textit{k} - \textit{r}) bit inferiori.

Il dizionario creato \textit{D} è un insieme di tuple \textlangle \textit{k}-mer, puntatore-dati-associati\textrangle, ordinate in ordine crescente rispetto al valore intero del \textit{k}-mer codificato, presi dalla lista di SNP in input. Viene costruita anche una hash table \textit{J} che mappa ogni numero intero senza segno di \textit{r} bit \textit{u} alla prima posizione in \textit{D} in cui vi è un \textit{k}-mer codificato i cui bit superiori sono maggiori o uguali a \textit{u}; si può notare la somiglianza con la struttura dati di LAVA. Infine, viene introdotto l'utilizzo di un Bloom Filter \textit{B} che contiene, per ogni i \textit{k}-mer presente, un elemento corrispondente ai (2\textit{k} - \textit{r}) bit inferiori; il numero di funzioni di hash del filtro è fissato ad uno per ridurre il tempo di hashing (vedi sezione \ref{BloomFilter}). Inoltre, dopo alcuni test viene fissata l'impostazione predefinita della dimensione del Bloom Filter a \textit{m} = 8\textit{n}, dove \textit{n} è il numero di valori distinti memorizzati, che corrisponde a un tasso teorico di falsi positivi di 0,118 \cite{sun-medvedev2018vargeno}.

\subsubsection{Algoritmo di VarGeno}

Per effettuare una query rispetto a un dato \textit{k}-mer \textit{K} e al suo vicinato \textit{N(K)}, si procede in due passaggi. 

\paragraph{Step 1} Inizialmente, si controlla se i (2\textit{k} - \textit{r}) bit inferiori di \textit{K} esistono in \textit{B}: in caso contrario, si abbandona la ricerca nel vicinato superiore poiché l'errore deve necessariamente essere nel blocco inferiore. In caso di risposta positiva, si cercano tutti i \textit{k}-mer nel vicinato superiore di \textit{K} e per ognuno di questi, utilizzando l'hash table \textit{J}, come accadeva in LAVA, si individua l'inizio e la fine del blocco nel dizionario \textit{D} con gli stessi bit superiori. Con la ricerca binaria tra gli elementi del blocco si individua, se esiste, il \textit{k}-mer corrispondente che appartiene al vicinato superiore; esso potrebbe non esistere poiché in \textit{B} potrebbero esserci falsi positivi (vedi sezione \ref{BloomFilter}).

\paragraph{Step 2} Viene poi eseguita una query nel vicinato inferiore, ricercando tutti i \textit{k}-mer che si trovano nel vicinato inferiore di \textit{K}. Dopo aver trovato, con l'utilizzo di \textit{J} gli indici di inizio e fine del blocco in \textit{D} con gli stessi \textit{r} bit superiori, se la dimensione del blocco è maggiore di una certa soglia \textit{t} (parametro \textit{t} = 25, determinato con vari test per ottenere il valore ottimale), per ogni \textit{k}-mer in \textit{N(K)} si esegue una ricerca binaria per trovare se esso esiste nel blocco. Se la dimensione del blocco è inferiore a \textit{t}, si esegue invece una scansione lineare, calcolando per ogni elemento la sua distanza di Hamming da \textit{K}, contando un match quando la distanza è al massimo 1. Il procedimento è illustrato anche nella Figura \ref{fig:vargeno}.

\begin{figure}[h!]
	\centering
  	\captionsetup{justification=centering}
 	\includegraphics[scale=.40]{images/vargeno-query.png}
  	\caption{Algoritmo di query, VarGeno.}
  	\label{fig:vargeno}
\end{figure}

\noindent
Confrontando i due procedimenti di indicizzazione e di query di LAVA e VarGeno, possiamo ritornare all'affermazione dell'introduzione: il framework VarGeno si basa su LAVA, aggiungendo l'uso del Bloom Filter e la scansione lineare se la dimensione del blocco individuato nel vicinato inferiore ha una dimensione ridotta. Queste euristiche consentono, nella pratica, di migliorare il tempo di esecuzione.

Per le query nel vicinato superiore, ogni \textit{k}-mer avrà bit superiori diversi e quindi richiederà un accesso separato a \textit{J} e un accesso casuale a \textit{D}. Probabilmente gli accessi a \textit{D}, per ogni \textit{k}-mer nel vicinato superiore, porteranno a un miss cache, poiché dobbiamo trovare esattamente i (2\textit{k} - \textit{r}) bit inferiori. Usando il Bloom Filter \textit{B}, proviamo a fare l'accesso solo per i \textit{k}-mer che rischiano di provocare un match ed eliminiamo molti accessi inutili e costosi.

Per le query nel vicinato inferiore, l'uso della scansione lineare quando la dimensione del blocco è piccola può comportare un miglioramento significativo delle prestazioni. Sebbene la ricerca binaria sia asintoticamente più veloce, il suo sovraccarico rispetto a una scansione lineare la rende più lenta quando la dimensione del blocco è ridotta: \cite{sun-medvedev2018vargeno} dimostra che il numero di blocchi con dimensione maggiore di \textit{t} è piccolo. 

\subparagraph{Osservazione} Dato \textit{n} il numero di \textit{k}-mer distinti memorizzati nel dizionario \textit{D} e \textit{b} il numero di blocchi, se i \textit{k}-mer codificati sono indipendenti l'uno dall'altro, la dimensione di un blocco in \textit{D} è almeno \textit{t} con probabilità al massimo $n/bt$. \\

\noindent
Anche se i \textit{k}-mer codificati non sono indipendenti perché molti si sovrappongono, le posizioni dei blocchi di due \textit{k}-mer codificati sono molto meno dipendenti.  Questo comporta che, per il genoma umano in cui il numero di \textit{k}-mer \textit{n} è circa 3 miliardi e \textit{b} è circa $2^{32}$, la probabilità che un blocco sia maggiore di \textit{t} = 25 è $<0,028$. Pertanto, l'algoritmo stima di ricorrere al metodo di ricerca binaria per meno del 3\% dei blocchi: in tutti gli altri casi viene utilizzata la scansione lineare che in questi casi è più veloce.\\

\noindent
Completiamo l'algoritmo; vengono costruiti due indici, uno per tutti i \textit{k}-mer presenti nella sequenza del genoma di riferimento e l'altro con i \textit{k}-mer da posizioni che si sovrappongono agli SNP della lista di SNP in input, con l'allele di riferimento sostituito da un allele alternato. VarGeno prende ciascuna read, la suddivide in \textit{k}-mer non sovrapposti come LAVA, e interroga i due indici, con il procedimento di query citato precedentemente. In aggiunta, vengono esplorati solo i vicini il cui punteggio di qualità per una determinata posizione è superiore a una soglia \textit{c}, altrimenti essi vengono saltati e non cercati, poiché è improbabile avere un errore di sequenziamento in una posizione con un alto punteggio di qualità \cite{sun-medvedev2018vargeno}: questa regola è un'aggiunta rispetto all'algoritmo di LAVA. Di default, VarGeno utilizza \textit{c} = 23, un valore intermedio, per ottenere prestazioni equilibrate. 

Dopo aver effettuato le ricerche dei i \textit{k}-mer da una read, VarGeno ne determina la singola posizione di mappatura. Una read è considerata mappata su una posizione del genoma di riferimento se valgono tutte le seguenti condizioni: (1) quella posizione ha il maggior numero di corrispondenze, (2) almeno due dei \textit{k}-mer che corrispondono provengono da posizioni diverse della read e (3) almeno un \textit{k}-mer non deve essere modificato (cioè presente nella read identico); se più di una posizione soddisfa questi criteri la read viene scartata. Un'ulteriore differenza con l'algoritmo di LAVA è in questi tre criteri: LAVA utilizza infatti solo il primo. Decisa la posizione di corrispondenza migliore della read sul genoma di riferimento, la read viene utilizzata per supportare l'allele di riferimento o l'allele alternato degli SNP all'interno della posizione corrispondente. Le informazioni sono memorizzate in una ``pileup table" che poi, con lo stesso modello probabilistico di LAVA, usando il teorema di Bayes, determina il genotipo più probabile per ciascun SNP nella lista.


\end{document}